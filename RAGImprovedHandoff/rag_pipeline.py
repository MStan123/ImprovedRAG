from pathlib import Path
import uuid
import re
from langchain_core.documents import Document
from langchain_classic.retrievers import ContextualCompressionRetriever
from langchain_core.retrievers import BaseRetriever
from retrieval import hybrid_summary_search
from reranker import summary_compressor
from llm import llm, fallback_llm
from semantic_cache import semantic_cache
from stats import stats
from logger_setup import setup_logger
from support_handoff import handoff
from conversation_manager import conversation_state
from chat_history_manager import chat_history
from feedback_manager import feedback_manager
from dwh_product_search import dwh_search
from useful_func import normalize_query, detect_lang, needs_human_handoff, contextualize_query, classify_query_with_llm,add_handoff_offer_to_response

logger = setup_logger()

def answer_query(
        query: str,
        user_id: str | None = None,
        session_id: str | None = None,
        history_last_n: int = 3
) -> tuple[str, list[Document], list[str], str]:
    """
    ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ RAG Ñ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ¼, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ feedback Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸ĞµĞ¹ DWH.

    Returns:
        (Ğ¾Ñ‚Ğ²ĞµÑ‚, reranked_docs, selected_files, feedback_id)
    """
    message_id = str(uuid.uuid4())
    normalized_query = normalize_query(query)

    user_lang = detect_lang(query)
    logger.info(f"ğŸŒ Detected language: {user_lang}")

    # 1. ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°
    contextualized_query = contextualize_query(normalized_query, user_id)
    logger.info(f"Processing query (user_id: {user_id or 'no-user'}, session: {session_id or 'no-session'}): {query}")
    if contextualized_query != normalized_query:
        logger.info(f"â†’ Contextualized to: {contextualized_query}")

    # ========== ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ: ĞĞ¶Ğ¸Ğ´Ğ°ĞµÑ‚ Ğ»Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ñ handoff? ==========
    # Ğ’ĞĞ–ĞĞ: Ğ­Ñ‚Ğ¾ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ¾ Ğ±Ñ‹Ñ‚ÑŒ ĞŸĞ•Ğ Ğ•Ğ” Ğ»ÑĞ±Ğ¾Ğ¹ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¾Ğ¹!
    pending_action = conversation_state.get_pending_action(user_id)

    if pending_action and pending_action["action_type"] == "handoff_confirmation":
        logger.info(f"ğŸ”” User {user_id} has pending handoff confirmation")

        # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ°
        user_response = conversation_state.parse_user_response(query)

        # ========== ĞŸĞĞ›Ğ¬Ğ—ĞĞ’ĞĞ¢Ğ•Ğ›Ğ¬ Ğ¡ĞĞ“Ğ›ĞĞ¡Ğ˜Ğ›Ğ¡Ğ¯ ==========
        if user_response == "yes":
            logger.info("âœ… User confirmed handoff, creating session...")

            # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· pending action
            action_data = pending_action["data"]

            # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ°
            extended_context = f"""â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¤– ĞĞ’Ğ¢ĞĞœĞĞ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜Ğ™ ĞŸĞ•Ğ Ğ•Ğ’ĞĞ”: ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ´Ğ¸Ğ» Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ¸ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ°
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“ ĞĞ Ğ˜Ğ“Ğ˜ĞĞĞ›Ğ¬ĞĞ«Ğ™ Ğ’ĞĞŸĞ ĞĞ¡:
{action_data["original_query"]}

ğŸ¤– ĞĞ¢Ğ’Ğ•Ğ¢ AI (ĞĞ• ĞŸĞĞœĞĞ“):
{action_data["ai_response"][:500]}{'...' if len(action_data["ai_response"]) > 500 else ''}

ğŸ“š ĞšĞĞĞ¢Ğ•ĞšĞ¡Ğ¢ Ğ˜Ğ— Ğ‘ĞĞ—Ğ« Ğ—ĞĞĞĞ˜Ğ™:
{action_data["context"][:800]}{'...' if len(action_data["context"]) > 800 else ''}

ğŸ’¡ ĞŸĞ Ğ˜Ğ§Ğ˜ĞĞ ĞŸĞ•Ğ Ğ•Ğ’ĞĞ”Ğ: ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ´Ğ¸Ğ» Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ½Ğ° Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ°
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

            # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ handoff ÑĞµÑÑĞ¸Ñ
            session_handoff_id = handoff.create_session(
                query=action_data["contextualized_query"],
                context=extended_context,
                user_id=user_id,
                user_phone=None,
                user_name=None,
                user_email=None
            )

            # ĞÑ‡Ğ¸Ñ‰Ğ°ĞµĞ¼ pending action
            conversation_state.clear_pending_action(user_id)

            # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚
            response_lang = user_lang if user_lang else 'az'
            chat_url = f"http://localhost:8001/chat?session={session_handoff_id}"

            handoff_messages = {
                'ru': f"âœ… ĞÑ‚Ğ»Ğ¸Ñ‡Ğ½Ğ¾! Ğ¡Ğ¾ĞµĞ´Ğ¸Ğ½ÑÑ Ğ²Ğ°Ñ Ñ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼...\n\nğŸ« ĞĞ¾Ğ¼ĞµÑ€ Ğ¾Ğ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ: #{session_handoff_id[:8].upper()}\nâ±ï¸ Ğ¡Ñ€ĞµĞ´Ğ½ĞµĞµ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ñ: ~2-3 Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹\n\nğŸ”— Ğ§Ğ°Ñ‚ Ğ¾Ñ‚ĞºÑ€Ğ¾ĞµÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸:\n{chat_url}",
                'az': f"âœ… Æla! Sizi operatorla É™laqÉ™lÉ™ndirirÉ™m...\n\nğŸ« MÃ¼raciÉ™t nÃ¶mrÉ™si: #{session_handoff_id[:8].upper()}\nâ±ï¸ Orta gÃ¶zlÉ™mÉ™ vaxtÄ±: ~2-3 dÉ™qiqÉ™\n\nğŸ”— Ã‡at avtomatik aÃ§Ä±lacaq:\n{chat_url}",
            }

            final_response = handoff_messages.get(response_lang, handoff_messages['az'])

            stats.handoff_count += 1
            logger.info(f"âœ… HANDOFF CONFIRMED - Session: {session_handoff_id}")

            # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ feedback Ñ Ğ¿Ğ¾Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ handoff_triggered
            feedback_id = feedback_manager.create_pending_feedback(
                ticket_id=message_id,
                user_id=user_id,
                session_id=session_id,
                original_query=action_data["original_query"],
                contextualized_query=action_data["contextualized_query"],
                ai_response=final_response,
                category="handoff_confirmed",
                selected_files=[],
                from_cache=False,
                handoff_triggered=True
            )

            return final_response, [], [], None

        # ========== ĞŸĞĞ›Ğ¬Ğ—ĞĞ’ĞĞ¢Ğ•Ğ›Ğ¬ ĞĞ¢ĞšĞĞ—ĞĞ›Ğ¡Ğ¯ ==========
        elif user_response == "no":
            logger.info("âŒ User declined handoff, continuing conversation...")

            # ĞÑ‡Ğ¸Ñ‰Ğ°ĞµĞ¼ pending action
            conversation_state.clear_pending_action(user_id)

            # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚
            response_lang = user_lang if user_lang else 'az'

            decline_messages = {
                'ru': "Ğ¥Ğ¾Ñ€Ğ¾ÑˆĞ¾, Ñ Ğ¿Ğ¾ÑÑ‚Ğ°Ñ€Ğ°ÑÑÑŒ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ Ğ²Ğ°Ğ¼ Ğ´Ğ°Ğ»ÑŒÑˆĞµ. Ğ§Ñ‚Ğ¾ Ğ±Ñ‹ Ğ²Ñ‹ Ñ…Ğ¾Ñ‚ĞµĞ»Ğ¸ ÑƒĞ·Ğ½Ğ°Ñ‚ÑŒ?",
                'az': "YaxÅŸÄ±, sizÉ™ kÃ¶mÉ™k etmÉ™yÉ™ davam edÉ™cÉ™yÉ™m. NÉ™ Ã¶yrÉ™nmÉ™k istÉ™rdiniz?",
            }

            final_response = decline_messages.get(response_lang, decline_messages['az'])

            # ĞĞ• ÑĞ¾Ğ·Ğ´Ğ°ĞµĞ¼ feedback Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºĞ°Ğ·Ğ° (ÑÑ‚Ğ¾ ÑĞ»ÑƒĞ¶ĞµĞ±Ğ½Ğ¾Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ)
            return final_response, [], [], None

        # ========== ĞĞ•ĞŸĞĞĞ¯Ğ¢ĞĞ«Ğ™ ĞĞ¢Ğ’Ğ•Ğ¢ ==========
        else:  # user_response == "unclear"
            logger.warning("âš ï¸ User response unclear, asking again...")

            response_lang = user_lang if user_lang else 'az'

            clarification_messages = {
                'ru': "Ğ˜Ğ·Ğ²Ğ¸Ğ½Ğ¸Ñ‚Ğµ, Ñ Ğ½Ğµ Ğ¿Ğ¾Ğ½ÑĞ» Ğ²Ğ°Ñˆ Ğ¾Ñ‚Ğ²ĞµÑ‚. ĞŸĞ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, Ğ¾Ñ‚Ğ²ĞµÑ‚ÑŒÑ‚Ğµ 'Ğ”Ğ°' Ğ¸Ğ»Ğ¸ 'ĞĞµÑ‚':\n\nĞ¥Ğ¾Ñ‚Ğ¸Ñ‚Ğµ ÑĞ¾ĞµĞ´Ğ¸Ğ½Ğ¸Ñ‚ÑŒÑÑ Ñ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼?",
                'az': "BaÄŸÄ±ÅŸlayÄ±n, cavabÄ±nÄ±zÄ± baÅŸa dÃ¼ÅŸmÉ™dim. ZÉ™hmÉ™t olmasa 'BÉ™li' vÉ™ ya 'Xeyr' cavabÄ± verin:\n\nOperatorla É™laqÉ™ saxlamaq istÉ™yirsiniz?",
            }

            final_response = clarification_messages.get(response_lang, clarification_messages['az'])

            return final_response, [], [], None

    # ========== ĞĞ‘Ğ«Ğ§ĞĞĞ¯ ĞĞ‘Ğ ĞĞ‘ĞĞ¢ĞšĞ Ğ—ĞĞŸĞ ĞĞ¡Ğ (ĞµÑĞ»Ğ¸ Ğ½ĞµÑ‚ pending action) ==========

    # ==================== SMART ROUTING ====================
    intent = classify_query_with_llm(contextualized_query)
    logger.info(f"ğŸ¯ Final routing decision: {intent}")

    # ==================== PRODUCT SEARCH (DWH) ====================
    if intent == "PRODUCT_SEARCH":
        logger.info("ğŸ›ï¸ Routing to DWH product search")

        # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ°
        search_params = extract_search_params(contextualized_query)

        # ĞŸĞ¾Ğ¸ÑĞº Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ¾Ğ² Ğ² DWH
        products = dwh_search.search_products(
            query=contextualized_query,
            only_in_stock=search_params.get('only_in_stock', True),
            min_price=search_params.get('min_price'),
            max_price=search_params.get('max_price'),
        )

        if products:
            logger.info(f"âœ… Found {len(products)} products in DWH")
            products_context = dwh_search.format_products_for_llm(products, user_lang.lower())

            # Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° LLM Ñ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ°Ğ¼Ğ¸
            messages = [
                {
                    "role": "system",
                    "content": (
                        f"You are a helpful shopping assistant for Birmarket marketplace.\n"
                        f"CRITICAL: You MUST respond ONLY in user's language.\n"
                        f"Answer questions about products based on the provided catalog data.\n"
                        f"Be friendly, concise, and helpful.\n"
                        f"ALWAYS mention: price, availability, seller name.\n"
                        f"If multiple products match - show up to {len(products)} products, but keep descriptions concise.\n"
                        f"Do not offer something extra at the end of the answer."
                    )
                },
                {
                    "role": "user",
                    "content": (
                        f"Available products:\n{products_context}\n\n"
                        f"User question: {query}\n\n"
                    )
                }
            ]

            try:
                response = llm.invoke(messages)
            except Exception:
                response = fallback_llm.invoke(messages)

            final_response = response.content

            # ĞŸĞ¾Ğ´ÑÑ‡ĞµÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²
            usage = response.response_metadata.get("usage", {})
            prompt_tokens = usage.get("prompt_tokens", 0)
            completion_tokens = usage.get("completion_tokens", 0)
            total_tokens = prompt_tokens + completion_tokens
            if total_tokens == 0:
                total_tokens = int(len((products_context + query + final_response).split()) * 1.3)

            stats.spent_tokens += total_tokens
            stats.llm_calls += 1

            if user_id:
                # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ
                chat_history.add_message(user_id, "user", query, metadata={"contextualized": contextualized_query})

                # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ°
                chat_history.add_message(user_id, "assistant", final_response,
                                         metadata={"tokens": total_tokens, "message_id": message_id})

            # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ feedback
            feedback_id = feedback_manager.create_pending_feedback(
                ticket_id=message_id,
                user_id=user_id,
                session_id=session_id,
                original_query=query,
                contextualized_query=contextualized_query,
                ai_response=final_response,
                category="product_search",
                selected_files=[],
                from_cache=False,
                handoff_triggered=False
            )

            logger.info("âœ… Product search completed successfully")
            # Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ Ğ¿ÑƒÑÑ‚Ñ‹Ğµ docs Ñ‚.Ğº. Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· DWH, Ğ½Ğµ Ğ¸Ğ· Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²
            return final_response, [], [], feedback_id
        else:
            # Ğ¢Ğ¾Ğ²Ğ°Ñ€Ñ‹ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹ - Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°ĞµĞ¼ÑÑ Ğ½Ğ° Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ RAG (Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¾Ğ±Ñ‰Ğ¸Ğ¹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ)
            logger.warning("âš ï¸ No products found in DWH, falling back to Knowledge Base")
            intent = "KNOWLEDGE_BASE"

    # ==================== ORDER STATUS ====================
    if intent == "ORDER_STATUS":
        logger.info("ğŸ“¦ Routing to ORDER_STATUS handler")
        # TODO: Ğ—Ğ´ĞµÑÑŒ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ° Ğ±Ñ‹Ñ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹ Ğ·Ğ°ĞºĞ°Ğ·Ğ¾Ğ²
        # ĞŸĞ¾ĞºĞ° Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· RAG Ğ¸Ğ»Ğ¸ handoff
        response = "Ğ”Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ÑÑ‚Ğ°Ñ‚ÑƒÑĞ° Ğ·Ğ°ĞºĞ°Ğ·Ğ°, Ğ¿Ğ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, ÑƒĞºĞ°Ğ¶Ğ¸Ñ‚Ğµ Ğ½Ğ¾Ğ¼ĞµÑ€ Ğ²Ğ°ÑˆĞµĞ³Ğ¾ Ğ·Ğ°ĞºĞ°Ğ·Ğ° Ğ¸Ğ»Ğ¸ ÑĞ¾ĞµĞ´Ğ¸Ğ½Ñ Ğ²Ğ°Ñ ÑĞ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸ÑÑ‚Ğ¾Ğ¼."

        return response, [], [], None

    # ==================== GENERAL (casual chat) ====================
    if intent == "GENERAL":
        logger.info("ğŸ’¬ General conversation detected")

        # ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ±ĞµĞ· retrieval Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹/Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ğ½Ğ¾ÑÑ‚ĞµĞ¹
        casual_patterns = [
            r'\b(Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚|Ğ·Ğ´Ñ€Ğ°Ğ²ÑÑ‚Ğ²ÑƒĞ¹|hi|hello|salam)\b',
            r'\b(ĞšĞ°Ğº Ğ´ĞµĞ»Ğ°?|ĞšĞ°Ğº Ğ²Ğ°ÑˆĞ¸ Ğ´ĞµĞ»Ğ°?|how are you?|necÉ™siz|necesiz)\b',
            r'\b(ÑĞ¿Ğ°ÑĞ¸Ğ±Ğ¾|thanks|tÉ™ÅŸÉ™kkÃ¼r|Ğ¼ĞµÑ€ÑĞ¸)\b',
            r'\b(Ğ¿Ğ¾ĞºĞ°|bye|goodbye|saÄŸol)\b'
        ]

        is_casual = any(re.search(p, query.lower()) for p in casual_patterns)

        if is_casual:
            messages = [{
                "role": "system",
                "content": (
                    f"You are a friendly Birmarket support assistant.\n"
                    f'If there is a choise between turkish or Azerbaijani - use JUST Azerbaijani.\n'
                    f"CRITICAL: Respond ONLY in user's language.\n"
                    f"Be warm and brief."
                )
            }, {
                "role": "user",
                "content": f"User message: {query}"
            }]

            try:
                response = llm.invoke(messages)
            except Exception:
                response = fallback_llm.invoke(messages)

            final_response = response.content

            return final_response, [], [], None

        # Ğ•ÑĞ»Ğ¸ Ğ½Ğµ ÑĞ¾Ğ²ÑĞµĞ¼ casual - Ğ¸Ñ‰ĞµĞ¼ Ğ² Knowledge Base
        intent = "KNOWLEDGE_BASE"

    # ==================== KNOWLEDGE BASE (RAG) ====================
    if intent == "KNOWLEDGE_BASE":
        logger.info("ğŸ“š Routing to Knowledge Base (RAG)")

        # 2. Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ retrieval
        summary_docs = hybrid_summary_search(contextualized_query, top_k=30)
        selected_files = [doc.metadata["file"] for doc in summary_docs]

        # 3. Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‡Ğ°Ğ½ĞºĞ¾Ğ²
        chunks_dir = Path("/home/user/PyCharmMiscProject/RAG/chunks")
        detailed_docs = []
        for file_name in selected_files:
            file_path = chunks_dir / file_name
            if file_path.exists():
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    detailed_docs.append(
                        Document(
                            page_content=content,
                            metadata={"source": file_name, "type": "detailed_chunk"}
                        )
                    )

        if not detailed_docs:
            response = "Ğš ÑĞ¾Ğ¶Ğ°Ğ»ĞµĞ½Ğ¸Ñ, Ğ½Ğµ Ğ½Ğ°ÑˆÑ‘Ğ» Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ²Ğ°ÑˆĞµĞ¼Ñƒ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑƒ."

            if user_id:
                chat_history.add_message(user_id, "user", query, metadata={"contextualized": contextualized_query})
                chat_history.add_message(user_id, "assistant", response, metadata={"no_relevant_docs": True})

            feedback_id = feedback_manager.create_pending_feedback(
                ticket_id=message_id,
                user_id=user_id,
                session_id=session_id,
                original_query=query,
                contextualized_query=contextualized_query,
                ai_response=response,
                category="knowledge_base",
                selected_files=[],
                from_cache=False,
                handoff_triggered=False
            )

            return response, [], selected_files, feedback_id

        # 4. Rerank Ñ‡ĞµÑ€ĞµĞ· FlashRank
        class SimpleRetriever(BaseRetriever):
            docs: list

            def _get_relevant_documents(self, query: str, **kwargs):
                return self.docs

        temp_retriever = SimpleRetriever(docs=detailed_docs)
        compression_retriever = ContextualCompressionRetriever(
            base_compressor=summary_compressor,
            base_retriever=temp_retriever
        )
        reranked_docs = compression_retriever.invoke(contextualized_query)

        # 5. Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚
        context = "\n\n".join(doc.page_content for doc in reranked_docs)

        # 6. Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ĞºÑÑˆ
        from_cache = False
        cached_doc = semantic_cache.retrieve_cached_response(contextualized_query)
        if cached_doc:
            stats.cache_hits += 1
            cached_tokens = cached_doc.metadata.get("tokens", 0)
            stats.saved_tokens += cached_tokens
            logger.info("âœ… From Semantic Cache")

            final_response = cached_doc.metadata["response"]
            from_cache = True

            if user_id:
                chat_history.add_message(user_id, "user", query, metadata={"contextualized": contextualized_query})
                chat_history.add_message(user_id, "assistant", cached_doc.metadata["response"],
                                         metadata={"from_cache": True})

            feedback_id = feedback_manager.create_pending_feedback(
                ticket_id=message_id,
                user_id=user_id,
                session_id=session_id,
                original_query=query,
                contextualized_query=contextualized_query,
                ai_response=final_response,
                category="knowledge_base",
                selected_files=selected_files,
                from_cache=True,
                handoff_triggered=False
            )

            return final_response, reranked_docs, selected_files, feedback_id

        dialog_history = []
        if user_id:
            recent = chat_history.get_history(user_id)[-6:]  # Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ 6 ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹
            for msg in recent:
                role = "User" if msg.role == "user" else "Assistant"
                dialog_history.append(f"{role}: {msg.content}")

        history_text = "\n\n".join(dialog_history) if dialog_history else "(no previous messages)"

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½Ğ¸Ğµ Ğ»Ğ¸ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ°
        is_followup = len(dialog_history) >= 2

        # Ğ‘Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚
        base_system = (
            "You are a friendly, warm, and professional AI assistant for Birmarket customer support â€” an online marketplace operating in Azerbaijan.\n\n"
            "CORE PRINCIPLES:\n"
            "â€¢ Answer strictly based on the provided context.\n"
            "â€¢ If the needed information is missing from the context â€” honestly say: 'Unfortunately, I don't have the exact information on this.'\n"
            "â€¢ NEVER invent, guess, or assume facts about Birmarket, prices, delivery times, product availability, policies, or any other details.\n\n"
            "ALLOWED SMALL TALK (answer naturally and kindly):\n"
            "â€¢ Greetings (salam, hi, hello, good day, etc.)\n"
            "â€¢ Questions about language ('Can I speak Russian?', 'AzÉ™rbaycanca?')\n"
            "â€¢ Thanks and polite phrases ('tÉ™ÅŸÉ™kkÃ¼r', 'thank you')\n"
            "â€¢ Casual questions like 'How are you?', 'What can you do?'\n"
            "â€¢ Requests to repeat or explain something again\n\n"
            "LANGUAGE & TONE RULES:\n"
            "â€¢ Be ready to answer ONLY in Russian, Azerbaijani â€” whichever the user chooses.\n"
            "â€¢ Speak politely, warmly, and concisely â€” like a friendly and competent shop assistant.\n"
            "â€¢ Keep answers short but complete.\n\n"
            "STRICT BOUNDARIES:\n"
            "â€¢ You are NOT a general-purpose AI. If the question is clearly outside Birmarket support â€” politely redirect.\n"
            "â€¢ Never discuss your own nature, model, training, xAI, Grok, etc."
        )

        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ°, ĞµÑĞ»Ğ¸ ÑÑ‚Ğ¾ follow-up
        if is_followup:
            continuation_instruction = (
                "\n\nIMPORTANT â€” THIS IS A CONTINUING CONVERSATION:\n"
                "- ALWAYS maintain context from previous messages.\n"
                "- If user says 'Ğ´Ğ°', 'Ñ€Ğ°ÑÑĞºĞ°Ğ¶Ğ¸ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½ĞµĞµ', 'ĞµÑ‰Ñ‘', 'more', 'tell me more', 'yes', etc. â€” "
                "continue and expand on the LAST topic you discussed.\n"
                "- Refer to your previous response when continuing.\n"
                "- Never forget what you just told the user.\n"
            )
            system_content = base_system + continuation_instruction
        else:
            system_content = base_system

        # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ
        messages = [
            {"role": "system", "content": system_content},
            {
                "role": "user",
                "content": f"""Recent chat history (for context):
                {history_text}

                Knowledge context:
                {context}

                Current question: {query}"""
            }
        ]

        # Ğ’Ñ‹Ğ·Ğ¾Ğ² LLM
        logger.info("ğŸ¤– Request to Azure OpenAI")
        stats.llm_calls += 1

        try:
            response = llm.invoke(messages)
        except Exception:
            response = fallback_llm.invoke(messages)

        # ĞŸĞ¾Ğ´ÑÑ‡Ñ‘Ñ‚ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²
        usage = response.response_metadata.get("usage", {})
        prompt_tokens = usage.get("prompt_tokens", 0)
        completion_tokens = usage.get("completion_tokens", 0)
        total_tokens = prompt_tokens + completion_tokens
        if total_tokens == 0:
            total_tokens = int(len((context + contextualized_query + response.content).split()) * 1.3)
        stats.spent_tokens += total_tokens

        final_response = response.content
        handoff_triggered = False

        # 8. Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ (Ğ”Ğ handoff Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸)
        if user_id:
            # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ
            chat_history.add_message(
                user_id,
                "user",
                query,
                metadata={"contextualized": contextualized_query}
            )

            # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ°
            chat_history.add_message(
                user_id,
                "assistant",
                final_response,
                metadata={
                    "tokens": total_tokens,
                    "message_id": message_id
                }
            )

        # ========== ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ ĞĞ£Ğ–Ğ•Ğ Ğ›Ğ˜ HANDOFF ==========
        handoff_decision = needs_human_handoff(final_response, context, query)

        if handoff_decision == "direct":
            # ĞŸĞ Ğ¯ĞœĞĞ™ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ - ÑĞ¾ĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ ÑÑ€Ğ°Ğ·Ñƒ Ğ±ĞµĞ· Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ñ
            logger.info("ğŸ”´ DIRECT handoff request detected")

            extended_context = f"""â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¤– ĞŸĞ Ğ¯ĞœĞĞ™ Ğ—ĞĞŸĞ ĞĞ¡ ĞĞ ĞĞŸĞ•Ğ ĞĞ¢ĞĞ Ğ
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“ Ğ—ĞĞŸĞ ĞĞ¡ ĞŸĞĞ›Ğ¬Ğ—ĞĞ’ĞĞ¢Ğ•Ğ›Ğ¯:
{query}

ğŸ“š ĞšĞĞĞ¢Ğ•ĞšĞ¡Ğ¢:
{context[:800]}{'...' if len(context) > 800 else ''}

ğŸ’¡ ĞŸĞ Ğ˜Ğ§Ğ˜ĞĞ: ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¸Ğ» ÑĞ²ÑĞ·ÑŒ Ñ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

            session_handoff_id = handoff.create_session(
                query=contextualized_query,
                context=extended_context,
                user_id=user_id,
                user_phone=None,
                user_name=None,
                user_email=None
            )

            response_lang = user_lang if user_lang else 'az'
            chat_url = f"http://localhost:8001/chat?session={session_handoff_id}"

            handoff_messages = {
                'ru': f"âœ… ĞšĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾! Ğ¡Ğ¾ĞµĞ´Ğ¸Ğ½ÑÑ Ğ²Ğ°Ñ Ñ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼...\n\nğŸ« ĞĞ¾Ğ¼ĞµÑ€ Ğ¾Ğ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ: #{session_handoff_id[:8].upper()}\nâ±ï¸ Ğ¡Ñ€ĞµĞ´Ğ½ĞµĞµ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ñ: ~2-3 Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹\n\nğŸ”— Ğ§Ğ°Ñ‚:\n{chat_url}",
                'az': f"âœ… ÆlbÉ™ttÉ™! Sizi operatorla É™laqÉ™lÉ™ndirirÉ™m...\n\nğŸ« MÃ¼raciÉ™t nÃ¶mrÉ™si: #{session_handoff_id[:8].upper()}\nâ±ï¸ Orta gÃ¶zlÉ™mÉ™ vaxtÄ±: ~2-3 dÉ™qiqÉ™\n\nğŸ”— Ã‡at:\n{chat_url}",
            }

            final_response = handoff_messages.get(response_lang, handoff_messages['az'])
            stats.handoff_count += 1
            handoff_triggered = True

        elif handoff_decision == "offer":
            # ĞŸĞ Ğ•Ğ”Ğ›ĞĞ–Ğ˜Ğ¢Ğ¬ handoff - ÑĞ¾Ğ·Ğ´Ğ°ĞµĞ¼ pending action
            logger.info("ğŸŸ¡ Handoff needed, creating confirmation request")

            # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğº Ğ¾Ñ‚Ğ²ĞµÑ‚Ñƒ
            final_response = add_handoff_offer_to_response(final_response, user_lang)

            # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ pending action
            conversation_state.create_handoff_confirmation(
                user_id=user_id,
                original_query=query,
                contextualized_query=contextualized_query,
                ai_response=final_response,
                context=context,
                ttl_minutes=10
            )

            logger.info(f"â³ Waiting for user confirmation (user_id: {user_id})")

        else:
            # Ğ’ÑÑ‘ ĞĞš, ĞºÑÑˆĞ¸Ñ€ÑƒĞµĞ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚
            semantic_cache.store_response(contextualized_query, final_response, total_tokens)
            stats.cached_responses += 1
            logger.info("âœ… Response cached")

        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ feedback
        feedback_id = feedback_manager.create_pending_feedback(
            ticket_id=message_id,
            user_id=user_id,
            session_id=session_id,
            original_query=query,
            contextualized_query=contextualized_query,
            ai_response=final_response,
            category="knowledge_base",
            selected_files=selected_files,
            from_cache=from_cache,
            handoff_triggered=handoff_triggered
        )

        return final_response, reranked_docs, selected_files, feedback_id

    # Ğ•ÑĞ»Ğ¸ Ğ¿Ğ¾Ğ¿Ğ°Ğ»Ğ¸ ÑÑĞ´Ğ° - Ñ‡Ñ‚Ğ¾-Ñ‚Ğ¾ Ğ¿Ğ¾ÑˆĞ»Ğ¾ Ğ½Ğµ Ñ‚Ğ°Ğº, Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ Ğ´ĞµÑ„Ğ¾Ğ»Ñ‚
    return "Ğ˜Ğ·Ğ²Ğ¸Ğ½Ğ¸Ñ‚Ğµ, Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ¾ÑˆĞ»Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ°. ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹Ñ‚Ğµ ĞµÑ‰Ğµ Ñ€Ğ°Ğ·.", [], [], None

__all__ = ["answer_query"]